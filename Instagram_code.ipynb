{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "supreme-citation",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Environment Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attempted-recording",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import csv\n",
    "import re\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import keras\n",
    "from nltk import word_tokenize\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers import *\n",
    "from keras.models import Model, load_model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer, InputSpec\n",
    "from keras import initializers\n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score\n",
    "from keras.optimizers import *\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "floral-spine",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Hyperparameter Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informational-hazard",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "MAX_SENT_LENGTH=30\n",
    "MAX_SENTS=140\n",
    "HISTORY_LENGTH=100\n",
    "MAX_EPOCHS=5\n",
    "MAX_TIMES=10\n",
    "BATCH_SIZE=32\n",
    "train_valtest_split=0.8\n",
    "# Download the instagram data from Homa Hosseinmardi et al. 2015. Analyzing labeled cyberbullying incidents on the instagram social network. \n",
    "Ins_data_path='data/ins/'\n",
    "normalUser_path='/data/ins/normalUser/'\n",
    "commonUser_path='/data/ins/commonUser/'\n",
    "# Download the pretrained word2vec embedding from the links below and replace them with your data paths\n",
    "w2vVocab_file='https://drive.google.com/file/d/1PqHeFg8QvXX_vGv54kgjRZK89XO6Gde0/view?usp=sharing'\n",
    "w2vVector_file='https://drive.google.com/file/d/1onEQw6yFAslUNlZeuviWp6_UyVyoB3KM/view?usp=sharing'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bright-lincoln",
   "metadata": {},
   "source": [
    "## Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outstanding-sunday",
   "metadata": {
    "code_folding": [
     1
    ]
   },
   "outputs": [],
   "source": [
    "# Convert time to hours\n",
    "def timeconvert(timestr,start_time):\n",
    "    ifpm=False\n",
    "    ntp=datetime.datetime.strptime(timestr, \"%Y-%m-%d %H:%M:%S\")\n",
    "    try:\n",
    "        otp=datetime.datetime.strptime(start_time, \"%Y-%m-%d %H:%M:%S\")\n",
    "    except:\n",
    "        start_time='2'+start_time\n",
    "        otp=datetime.datetime.strptime(start_time, \"%Y-%m-%d %H:%M:%S\")\n",
    "    delta=ntp-otp\n",
    "    hours=delta.days*24+delta.seconds/3600\n",
    "    return hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "limiting-trout",
   "metadata": {
    "code_folding": [
     1,
     41,
     57
    ]
   },
   "outputs": [],
   "source": [
    "# Map pretrained word embedding\n",
    "def generate_indices_embedding(tokenized_text):\n",
    "    w2vVocab=np.load(w2vVocab_file,allow_pickle=True).item()\n",
    "    w2vVector=np.load(w2vVector_file)\n",
    "\n",
    "    word_indices_count={'PADDING':[0,99999]}\n",
    "    word_indices={'PADDING':0}\n",
    "    embedding_index={'PADDING':0}\n",
    "    \n",
    "    for sessions in tokenized_text:\n",
    "        for sentence in sessions:\n",
    "            for word in sentence:\n",
    "                if word not in word_indices_count:\n",
    "                    word_indices_count[word]=[len(word_indices_count),1]\n",
    "                else:\n",
    "                    word_indices_count[word][1]+=1\n",
    "    for word in word_indices_count.keys():\n",
    "        word_indices[word]=len(word_indices)\n",
    "\n",
    "    count=0\n",
    "    for word in word_indices.keys():\n",
    "        if word in w2vVocab.keys():\n",
    "            count+=1 \n",
    "        elif word_indices_count[word][1]>3:\n",
    "            count+=1\n",
    "    lister=np.zeros((count,400),dtype='float32')\n",
    "        \n",
    "    for word in word_indices.keys():\n",
    "        if word in w2vVocab.keys() and word!='PADDING':\n",
    "            embedding_index[word]=len(embedding_index)\n",
    "            lister[embedding_index[word]]=w2vVector[w2vVocab[word]]\n",
    "    reference=lister[:len(embedding_index)-1]\n",
    "    mu=np.mean(reference, axis=0)\n",
    "    Sigma=np.cov(reference.T)\n",
    "    \n",
    "    for word in word_indices_count.keys():\n",
    "        if word not in embedding_index.keys() and word_indices_count[word][1]>3:\n",
    "            embedding_index[word]=len(embedding_index)\n",
    "            lister[embedding_index[word]]=np.random.multivariate_normal(mu, Sigma, 1)\n",
    "    \n",
    "    return embedding_index,lister\n",
    "def generate_word_index(session_tokens,embedding_index):\n",
    "    text_word_indices=[]\n",
    "    for session in session_tokens:\n",
    "        session_index=[]\n",
    "        for sentence in session:\n",
    "            text_word_index=[]\n",
    "            for word in sentence:\n",
    "                if word in embedding_index:\n",
    "                    text_word_index.append(embedding_index[word])\n",
    "                else:\n",
    "                    text_word_index.append(embedding_index['PADDING'])\n",
    "            text_word_index=text_word_index[:MAX_SENT_LENGTH]+(MAX_SENT_LENGTH-len(text_word_index))*[0]\n",
    "            session_index.append(text_word_index)\n",
    "        session_index=session_index[:MAX_SENTS]+(MAX_SENTS-len(session_index))*[[0]*MAX_SENT_LENGTH]\n",
    "        text_word_indices.append(session_index)\n",
    "    return text_word_indices\n",
    "def generate_history_word_index(session_tokens,embedding_index):\n",
    "    text_word_indices=[]\n",
    "    for session in session_tokens:\n",
    "        session_index=[]\n",
    "        for user in session:\n",
    "            user_index=[]\n",
    "            for sentence in user:\n",
    "                for word in sentence:\n",
    "                    if word in embedding_index:\n",
    "                        user_index.append(embedding_index[word])\n",
    "                    else:\n",
    "                        user_index.append(embedding_index['PADDING'])\n",
    "            user_index=user_index[:HISTORY_LENGTH]+(HISTORY_LENGTH-len(user_index))*[0]\n",
    "            session_index.append(user_index)\n",
    "        session_index=session_index[:MAX_SENTS]+(MAX_SENTS-len(session_index))*[[0]*HISTORY_LENGTH]\n",
    "        text_word_indices.append(session_index)\n",
    "    return text_word_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "anonymous-heavy",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Load session data\n",
    "dataDict=[]\n",
    "with open(Ins_data_path+'sessions_0plus_to_10_metadata.csv', 'r',encoding='unicode_escape') as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for row in reader:\n",
    "        dataDict.append(row)\n",
    "with open(Ins_data_path+'sessions_10plus_to_40_metadata.csv', 'r',encoding='unicode_escape') as f:\n",
    "    reader = csv.DictReader((line.replace('\\0','') for line in f))\n",
    "    for row in reader:\n",
    "        dataDict.append(row)\n",
    "with open(Ins_data_path+'sessions_40plus_metadata.csv', 'r',encoding='unicode_escape') as f:\n",
    "    reader = csv.DictReader((line.replace('\\0','') for line in f))\n",
    "    for row in reader:\n",
    "        dataDict.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "least-radius",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Remove redundant session info\n",
    "removeList=['_golden','_unit_state','_unit_id','_trusted_judgments','_last_judgment_at','img_url']\n",
    "for row in dataDict:\n",
    "    row['cptn_time']=row['cptn_time'].split('Media posted at ')[-1].strip()\n",
    "    for keys in list(row):\n",
    "        if (keys in removeList) or (keys[:4]=='clmn' and row[keys]=='empety'):\n",
    "            del row[keys]\n",
    "        elif keys[:4]=='clmn':\n",
    "            row[keys]=row[keys].replace('<font color=\"#0066CC\">',\"\")\n",
    "            row[keys]=row[keys].replace('</font>',\"\")\n",
    "            row[keys]=row[keys].replace('(created at:','(created_at:')\n",
    "            row[keys]=row[keys].split('(created_at:')\n",
    "            row[keys]=[row[keys][0].strip(),row[keys][1].strip(')')]\n",
    "            row[keys]=[row[keys][0].split('   ')[0],row[keys][0].split('   ')[1],row[keys][1]]\n",
    "            row[keys][0]=row[keys][0].lower()\n",
    "            row[keys][1]=row[keys][1].lower()\n",
    "            new_str=re.sub(r'[\\x80-\\xFF]+','',row[keys][1])\n",
    "            if new_str!=row[keys][1]:\n",
    "                row[keys][1]=re.sub('\\_*','',new_str)\n",
    "            row[keys][1]=word_tokenize(row[keys][1])\n",
    "            row[keys][2]=timeconvert(row[keys][2],row['cptn_time'])\n",
    "    row['likes']=row['likes'].split('\\n\\n ')[0]\n",
    "    row['owner_id']=row['owner_id'].replace('<font color=\"#0066CC\">',\"\")\n",
    "    row['owner_id']=row['owner_id'].replace('</font>  ',\"\")\n",
    "    new_cptn=re.sub(r'[\\x80-\\xFF]+','',row['owner_cmnt'])\n",
    "    if new_cptn!=row['owner_cmnt']:\n",
    "        row['owner_cmnt']=re.sub('\\_*','',new_cptn)\n",
    "    row['owner_cmnt']=word_tokenize(row['owner_cmnt'].lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dressed-midwest",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Load user history data\n",
    "userhistory=[]\n",
    "pattern = re.compile(r'^userID:.*\\susername:.*\\scommentText:.*$') \n",
    "for filename in os.listdir(normalUser_path):\n",
    "    with open(normalUser_path+filename,'r',encoding=\"utf8\") as f:\n",
    "        lines=f.readlines()\n",
    "        for line in lines:\n",
    "            sub = pattern.search(line) \n",
    "            try:\n",
    "                sub.group()\n",
    "                userhistory.append(line)\n",
    "            except:\n",
    "                pass\n",
    "for filename in os.listdir(commonUser_path):\n",
    "    with open(commonUser_path+filename,'r',encoding=\"utf8\") as f:\n",
    "        lines=f.readlines()\n",
    "        for line in lines:\n",
    "            sub = pattern.search(line) \n",
    "            try:\n",
    "                sub.group()\n",
    "                userhistory.append(line)\n",
    "            except:\n",
    "                pass\n",
    "for person in userhistory.keys():\n",
    "    for i in range(len(userhistory[person])):\n",
    "        userhistory[person][i]= word_tokenize(userhistory[person][i].lower())\n",
    "        if userhistory[person][i][0]=='@':\n",
    "            userhistory[person][i]=userhistory[person][i][2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "norwegian-hayes",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Construct input session data according to time\n",
    "session_labels=[]\n",
    "session_tokens=[]\n",
    "session_histories=[]\n",
    "session_times=[]\n",
    "for row in dataDict:\n",
    "    if row['question1']=='noneAgg' and row['question2']=='noneBll':\n",
    "        session_labels.append(0)\n",
    "    elif row['question1']=='aggression' and row['question2']=='noneBll':\n",
    "        session_labels.append(0)\n",
    "    else:\n",
    "        session_labels.append(1)\n",
    "    \n",
    "    row_tokens=[]\n",
    "    row_times=[]\n",
    "    row_history=[]\n",
    "    owner_ut=[row['owner_id']]+row['owner_cmnt']\n",
    "    row_tokens.append(owner_ut)\n",
    "    if row['owner_id'] in userhistory.keys():\n",
    "        row_history.append(userhistory[row['owner_id']][:30])\n",
    "    else:\n",
    "        row_history.append([])\n",
    "    row_times.append(0)\n",
    "    for keys in list(row):\n",
    "        if keys[:4]=='clmn':\n",
    "            row_tokens.append([row[keys][0]]+row[keys][1])\n",
    "            row_times.append(row[keys][2])\n",
    "            if row[keys][0] in userhistory.keys():\n",
    "                row_history.append(userhistory[row[keys][0]][:30])\n",
    "            else:\n",
    "                row_history.append([])\n",
    "    sorted_row_times=[]\n",
    "    sorted_row_tokens=[]\n",
    "    sorted_row_history=[]\n",
    "    \n",
    "    mintime=row_times[np.argsort(row_times)[0]]\n",
    "    for i in np.argsort(row_times):\n",
    "        if mintime<0:\n",
    "            sorted_row_times.append(row_times[i]-mintime)\n",
    "        else:\n",
    "            sorted_row_times.append(row_times[i])\n",
    "        sorted_row_tokens.append(row_tokens[i])\n",
    "        sorted_row_history.append(row_history[i])\n",
    "    \n",
    "    sorted_row_times=sorted_row_times[:MAX_SENTS]+(MAX_SENTS-len(sorted_row_times))*[0]\n",
    "    session_times.append(sorted_row_times)\n",
    "    session_tokens.append(sorted_row_tokens)\n",
    "    session_histories.append(sorted_row_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "burning-prevention",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Tokenize input sessions\n",
    "embedding_index,lister=generate_indices_embedding(session_tokens)\n",
    "session_indices=generate_word_index(session_tokens,embedding_index)\n",
    "session_history_indices=generate_history_word_index(session_histories,embedding_index)\n",
    "\n",
    "session_indices=np.array(session_indices)\n",
    "session_times=np.array(session_times)\n",
    "session_labels=np.array(session_labels)\n",
    "session_history_indices=np.array(session_history_indices)\n",
    "session_times=np.expand_dims(session_times, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "miniature-pakistan",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dangerous-switzerland",
   "metadata": {
    "code_folding": [
     0,
     13,
     83,
     85
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class lambdaLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(lambdaLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super(lambdaLayer, self).build(input_shape)  \n",
    "\n",
    "    def call(self, x):\n",
    "        result =x[0]*x[2]+x[1]*x[3]\n",
    "        return result\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[2]\n",
    "class TimeAtt(Layer):\n",
    "    def __init__(self, nb_head, size_per_head, **kwargs):\n",
    "        self.nb_head = nb_head\n",
    "        self.size_per_head = size_per_head\n",
    "        self.output_dim = nb_head*size_per_head\n",
    "        super(TimeAtt, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.WQ = self.add_weight(name='WQ', \n",
    "                                  shape=(input_shape[0][-1], self.output_dim),\n",
    "                                  initializer='glorot_uniform',\n",
    "                                  trainable=True)\n",
    "        self.WK = self.add_weight(name='WK', \n",
    "                                  shape=(input_shape[1][-1], self.output_dim),\n",
    "                                  initializer='glorot_uniform',\n",
    "                                  trainable=True)\n",
    "        self.WV = self.add_weight(name='WV', \n",
    "                                  shape=(input_shape[2][-1], self.output_dim),\n",
    "                                  initializer='glorot_uniform',\n",
    "                                  trainable=True)\n",
    "        \n",
    "        super(TimeAtt, self).build(input_shape)\n",
    "        \n",
    "    def Mask(self, inputs, seq_len, mode='mul'):\n",
    "        if seq_len == None:\n",
    "            return inputs\n",
    "        else:\n",
    "            mask = K.one_hot(seq_len[:,0], K.shape(inputs)[1])\n",
    "            mask = 1 - K.cumsum(mask, 1)\n",
    "            for _ in range(len(inputs.shape)-2):\n",
    "                mask = K.expand_dims(mask, 2)\n",
    "            if mode == 'mul':\n",
    "                return inputs * mask\n",
    "            if mode == 'add':\n",
    "                return inputs - (1 - mask) * 1e12\n",
    "                \n",
    "    def call(self, x):\n",
    "        if len(x) == 4:\n",
    "            Q_seq,K_seq,V_seq,T_seq = x\n",
    "            Q_len,V_len = None,None\n",
    "        elif len(x) == 5:\n",
    "            Q_seq,K_seq,V_seq,Q_len,V_len = x\n",
    "        T1_seq = K.repeat_elements(T_seq,K.int_shape(T_seq)[1],2)\n",
    "        T2_seq = K.permute_dimensions(T1_seq, (0,2,1))\n",
    "        T1_seq = K.reshape(T1_seq, (-1,1,K.shape(T1_seq)[1],K.shape(T1_seq)[2]))\n",
    "        T2_seq = K.reshape(T2_seq, (-1,1,K.shape(T2_seq)[1],K.shape(T2_seq)[2]))\n",
    "        T_seq = add([T1_seq, -T2_seq])\n",
    "        Q_seq = K.dot(Q_seq, self.WQ)\n",
    "        Q_seq = K.reshape(Q_seq, (-1, K.shape(Q_seq)[1], self.nb_head, self.size_per_head))\n",
    "        Q_seq = K.permute_dimensions(Q_seq, (0,2,1,3))\n",
    "        K_seq = K.dot(K_seq, self.WK)\n",
    "        K_seq = K.reshape(K_seq, (-1, K.shape(K_seq)[1], self.nb_head, self.size_per_head))\n",
    "        K_seq = K.permute_dimensions(K_seq, (0,2,1,3))\n",
    "        V_seq = K.dot(V_seq, self.WV)\n",
    "        V_seq = K.reshape(V_seq, (-1, K.shape(V_seq)[1], self.nb_head, self.size_per_head))\n",
    "        V_seq = K.permute_dimensions(V_seq, (0,2,1,3))\n",
    "        A = K.batch_dot(Q_seq, K_seq, axes=[3,3]) / self.size_per_head**0.5\n",
    "        A = K.permute_dimensions(A, (0,3,2,1))\n",
    "        A = self.Mask(A, V_len, 'add')\n",
    "        A = K.permute_dimensions(A, (0,3,2,1))\n",
    "        A = add([T_seq, A])\n",
    "        A = K.softmax(A)\n",
    "        O_seq = K.batch_dot(A, V_seq, axes=[3,2])\n",
    "        O_seq = K.permute_dimensions(O_seq, (0,2,1,3))\n",
    "        O_seq = K.reshape(O_seq, (-1, K.shape(O_seq)[1], self.output_dim))\n",
    "        O_seq = self.Mask(O_seq, Q_len, 'mul')\n",
    "        return O_seq\n",
    "        \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0][0], input_shape[0][1], self.output_dim)\n",
    "def slice(x,index):\n",
    "    return x[:,index,:]\n",
    "class scoreHistory(keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        true = test_session_labels\n",
    "        predictions = model.predict([test_session_indices,test_session_times,test_session_history_indices], batch_size=32, verbose=1)\n",
    "        auc=roc_auc_score(true, predictions[:,1])\n",
    "        predictions = np.argmax(predictions, axis=1)\n",
    "        cr = classification_report(true, predictions,digits=4)\n",
    "        acc_score=accuracy_score(true, predictions)\n",
    "        print(cr)\n",
    "        if times not in results.keys():\n",
    "            results[times]=[float(cr.split()[10]),float(cr.split()[11]),float(cr.split()[12]),auc,0]\n",
    "        else:\n",
    "            if float(cr.split()[12])>results[times][2]:\n",
    "                results[times]=[float(cr.split()[10]),float(cr.split()[11]),float(cr.split()[12]),auc,epoch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aware-advice",
   "metadata": {
    "code_folding": [
     1
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "results={}\n",
    "for times in range(MAX_TIMES):\n",
    "    # Shuffle data\n",
    "    indices = np.arange(len(session_indices))\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    train_session_indices=session_indices[indices[:int(train_valtest_split*len(indices))]]\n",
    "    train_session_times=session_times[indices[:int(train_valtest_split*len(indices))]]\n",
    "    train_session_labels=session_labels[indices[:int(train_valtest_split*len(indices))]]\n",
    "    train_session_history_indices=session_history_indices[indices[:int(train_valtest_split*len(indices))]]\n",
    "\n",
    "    test_session_indices=session_indices[indices[int(train_valtest_split*len(indices)):]]\n",
    "    test_session_times=session_times[indices[int(train_valtest_split*len(indices)):]]\n",
    "    test_session_labels=session_labels[indices[int(train_valtest_split*len(indices)):]]\n",
    "    test_session_history_indices=session_history_indices[indices[int(train_valtest_split*len(indices)):]]\n",
    "        \n",
    "    title_input = Input(shape=(MAX_SENT_LENGTH,), dtype='int32')\n",
    "    wordEmb=Embedding(len(lister),400, weights=[lister],trainable=True)\n",
    "    titles = wordEmb(title_input)\n",
    "    d_titles=Dropout(0.2)(titles)\n",
    "    sentenceGRU=Bidirectional(GRU(32,return_sequences=True))\n",
    "    title_attrep=sentenceGRU(d_titles)\n",
    "    dense1=Dense(32,activation='tanh')\n",
    "    dense2=Dense(1)\n",
    "    attention = dense1(title_attrep)\n",
    "    attention = Flatten()(dense2(attention))\n",
    "    attention_weight = Activation('softmax')(attention)\n",
    "    title_att=keras.layers.Dot((1, 1))([title_attrep, attention_weight])\n",
    "    sentEncodert = Model(title_input,title_att)\n",
    "    \n",
    "    history_input = Input((HISTORY_LENGTH,), dtype='int32')\n",
    "    histories = wordEmb(history_input)\n",
    "    d_histories=Dropout(0.2)(histories)\n",
    "    his_attrep=sentenceGRU(d_histories)\n",
    "    hattention = dense1(his_attrep)\n",
    "    hattention = Flatten()(dense2(hattention))\n",
    "    hattention_weight = Activation('softmax')(hattention)\n",
    "    hisT_att=keras.layers.Dot((1, 1))([his_attrep, hattention_weight])\n",
    "    histEncodert = Model(history_input,hisT_att)\n",
    "    \n",
    "    session_input = Input((MAX_SENTS,MAX_SENT_LENGTH,), dtype='int32')\n",
    "    session_encoded= TimeDistributed(sentEncodert)(session_input)\n",
    "    session_time_input = Input((MAX_SENTS,1), dtype='float32')\n",
    "    time_dense1=Dense(1,bias=True,activation='sigmoid')\n",
    "    time_encoded=time_dense1(session_time_input)\n",
    "    session_history_input = Input((MAX_SENTS,HISTORY_LENGTH,), dtype='int32')\n",
    "    session_history_encoded= TimeDistributed(histEncodert)(session_history_input)\n",
    "    d_session_encoded=Dropout(0.2)(session_encoded)\n",
    "    d_session_history_encoded=Dropout(0.2)(session_history_encoded)\n",
    "    session_rep=Bidirectional(GRU(64,return_sequences=True))(d_session_encoded)\n",
    "    rsession_rep=TimeAtt(1,64)([session_rep,session_rep,session_rep,time_encoded])\n",
    "    his_dense=Dense(1,bias=True)\n",
    "    w_rsession_rep=his_dense(rsession_rep)\n",
    "    w_d_session_history_encoded=his_dense(d_session_history_encoded)\n",
    "    lamb=lambdaLayer()\n",
    "    rsession_rep=lamb([w_rsession_rep,w_d_session_history_encoded,rsession_rep,d_session_history_encoded])\n",
    "    satt= Dense(32,activation='tanh')(rsession_rep)\n",
    "    satt= Flatten()(Dense(1)(satt))\n",
    "    sattention_weight = Activation('softmax')(satt)\n",
    "    session_vec=keras.layers.Dot((1, 1))([rsession_rep, sattention_weight])\n",
    "    predict_vec=Dense(2,activation='softmax')(session_vec)\n",
    "    model = Model([session_input,session_time_input,session_history_input],predict_vec)\n",
    "    model.compile(loss=['categorical_crossentropy'], optimizer=Adam(lr=0.001), metrics=['acc'])\n",
    "    scorehistory = scoreHistory()\n",
    "    model.fit([train_session_indices,train_session_times,train_session_history_indices],to_categorical(train_session_labels),batch_size=BATCH_SIZE,\n",
    "              callbacks=[scorehistory],epochs=MAX_EPOCHS,shuffle=True,class_weight={0:1,1:1})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
