{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cardiac-valentine",
   "metadata": {},
   "source": [
    "## Environment Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "civil-metadata",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import csv\n",
    "import re\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import keras\n",
    "from nltk import word_tokenize\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers import *\n",
    "from keras.models import Model, load_model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer, InputSpec\n",
    "from keras import initializers\n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score\n",
    "from keras.optimizers import *\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "devoted-cuisine",
   "metadata": {},
   "source": [
    "## Hyperparameter Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "automated-supplier",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SENT_LENGTH=30\n",
    "MAX_SENTS=140\n",
    "HISTORY_LENGTH=100\n",
    "MAX_EPOCHS=7\n",
    "MAX_TIMES=20\n",
    "BATCH_SIZE=16\n",
    "train_valtest_split=0.8\n",
    "# Download the vine data from Rahat Ibn Rafiq et al. 2015. Careful what you share in six seconds: Detecting cyberbullying instances in Vine. \n",
    "Vine_data_path='data/vine/'\n",
    "# Download the pretrained word2vec embedding from the links below and replace them with your data paths\n",
    "w2vVocab_file='https://drive.google.com/file/d/1PqHeFg8QvXX_vGv54kgjRZK89XO6Gde0/view?usp=sharing'\n",
    "w2vVector_file='https://drive.google.com/file/d/1onEQw6yFAslUNlZeuviWp6_UyVyoB3KM/view?usp=sharing'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "failing-space",
   "metadata": {},
   "source": [
    "## Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "geological-production",
   "metadata": {
    "code_folding": [
     1
    ]
   },
   "outputs": [],
   "source": [
    "# Convert time to hours\n",
    "def timeconvert(timestr,start_time):\n",
    "    ifpm=False\n",
    "    ntp=datetime.datetime.strptime(timestr, \"%Y-%m-%d %H:%M:%S\")\n",
    "    try:\n",
    "        otp=datetime.datetime.strptime(start_time, \"%Y-%m-%d %H:%M:%S\")\n",
    "    except:\n",
    "        start_time='2'+start_time\n",
    "        otp=datetime.datetime.strptime(start_time, \"%Y-%m-%d %H:%M:%S\")\n",
    "    delta=ntp-otp\n",
    "    hours=delta.days*24+delta.seconds/3600\n",
    "    return hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brave-composite",
   "metadata": {
    "code_folding": [
     1,
     41,
     57
    ]
   },
   "outputs": [],
   "source": [
    "# Map pretrained word embedding\n",
    "def generate_indices_embedding(tokenized_text):\n",
    "    w2vVocab=np.load(w2vVocab_file,allow_pickle=True).item()\n",
    "    w2vVector=np.load(w2vVector_file)\n",
    "\n",
    "    word_indices_count={'PADDING':[0,99999]}\n",
    "    word_indices={'PADDING':0}\n",
    "    embedding_index={'PADDING':0}\n",
    "    \n",
    "    for sessions in tokenized_text:\n",
    "        for sentence in sessions:\n",
    "            for word in sentence:\n",
    "                if word not in word_indices_count:\n",
    "                    word_indices_count[word]=[len(word_indices_count),1]\n",
    "                else:\n",
    "                    word_indices_count[word][1]+=1\n",
    "    for word in word_indices_count.keys():\n",
    "        word_indices[word]=len(word_indices)\n",
    "\n",
    "    count=0\n",
    "    for word in word_indices.keys():\n",
    "        if word in w2vVocab.keys():\n",
    "            count+=1 \n",
    "        elif word_indices_count[word][1]>3:\n",
    "            count+=1\n",
    "    lister=np.zeros((count,400),dtype='float32')\n",
    "        \n",
    "    for word in word_indices.keys():\n",
    "        if word in w2vVocab.keys() and word!='PADDING':\n",
    "            embedding_index[word]=len(embedding_index)\n",
    "            lister[embedding_index[word]]=w2vVector[w2vVocab[word]]\n",
    "    reference=lister[:len(embedding_index)-1]\n",
    "    mu=np.mean(reference, axis=0)\n",
    "    Sigma=np.cov(reference.T)\n",
    "    \n",
    "    for word in word_indices_count.keys():\n",
    "        if word not in embedding_index.keys() and word_indices_count[word][1]>3:\n",
    "            embedding_index[word]=len(embedding_index)\n",
    "            lister[embedding_index[word]]=np.random.multivariate_normal(mu, Sigma, 1)\n",
    "    \n",
    "    return embedding_index,lister\n",
    "def generate_word_index(session_tokens,embedding_index):\n",
    "    text_word_indices=[]\n",
    "    for session in session_tokens:\n",
    "        session_index=[]\n",
    "        for sentence in session:\n",
    "            text_word_index=[]\n",
    "            for word in sentence:\n",
    "                if word in embedding_index:\n",
    "                    text_word_index.append(embedding_index[word])\n",
    "                else:\n",
    "                    text_word_index.append(embedding_index['PADDING'])\n",
    "            text_word_index=text_word_index[:MAX_SENT_LENGTH]+(MAX_SENT_LENGTH-len(text_word_index))*[0]\n",
    "            session_index.append(text_word_index)\n",
    "        session_index=session_index[:MAX_SENTS]+(MAX_SENTS-len(session_index))*[[0]*MAX_SENT_LENGTH]\n",
    "        text_word_indices.append(session_index)\n",
    "    return text_word_indices\n",
    "def generate_history_word_index(session_tokens,embedding_index):\n",
    "    text_word_indices=[]\n",
    "    for session in session_tokens:\n",
    "        session_index=[]\n",
    "        for user in session:\n",
    "            user_index=[]\n",
    "            for sentence in user:\n",
    "                for word in sentence:\n",
    "                    if word in embedding_index:\n",
    "                        user_index.append(embedding_index[word])\n",
    "                    else:\n",
    "                        user_index.append(embedding_index['PADDING'])\n",
    "            user_index=user_index[:HISTORY_LENGTH]+(HISTORY_LENGTH-len(user_index))*[0]\n",
    "            session_index.append(user_index)\n",
    "        session_index=session_index[:MAX_SENTS]+(MAX_SENTS-len(session_index))*[[0]*HISTORY_LENGTH]\n",
    "        text_word_indices.append(session_index)\n",
    "    return text_word_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modular-browse",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Load session data\n",
    "dataDict=[]\n",
    "with open(Vine_data_path+'vine_labeled_cyberbullying_data.csv', 'r',encoding='unicode_escape') as f:\n",
    "    reader = csv.DictReader((line.replace('\\0','') for line in f))\n",
    "    for row in reader:\n",
    "        dataDict.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "provincial-lingerie",
   "metadata": {
    "code_folding": [
     2
    ]
   },
   "outputs": [],
   "source": [
    "# Remove redundant session info\n",
    "removeList=['_golden','_unit_state','_unit_id','_trusted_judgments','_last_judgment_at','img_url']\n",
    "for row in dataDict:\n",
    "    row['creationtime']=row['creationtime'].split('posted at:')[-1].strip()\n",
    "    row['creationtime']=row['creationtime'].replace('.000000','')\n",
    "    row['creationtime']=row['creationtime'].replace('T',' ')\n",
    "    for keys in list(row):\n",
    "        if (keys in removeList) or (keys[:4]=='colu' and row[keys]=='empty'):\n",
    "            del row[keys]\n",
    "        elif keys[:4]=='colu':\n",
    "            row[keys]=row[keys].replace('<font color=\"#0066CC\">',\"\")\n",
    "            row[keys]=row[keys].replace('</font>::',\"&&&&&\")\n",
    "            row[keys]=row[keys].replace('(created at:','(created_at:')\n",
    "            row[keys]=row[keys].split('(created_at:')\n",
    "            if len(row[keys])>1:\n",
    "                row[keys]=[row[keys][0].strip(),row[keys][1].strip(')')]\n",
    "                row[keys][1]=row[keys][1].replace('.000000','')\n",
    "                row[keys]=[row[keys][0].split(\"&&&&&\")[0],row[keys][0].split(\"&&&&&\")[1],row[keys][1].replace('T',' ')]\n",
    "                row[keys][0]=row[keys][0].lower()\n",
    "                row[keys][1]=row[keys][1].lower()\n",
    "                new_str=re.sub(r'[\\x80-\\xFF]+','',row[keys][1])\n",
    "                if new_str!=row[keys][1]:\n",
    "                    row[keys][1]=re.sub('\\_*','',new_str)\n",
    "                row[keys][1]=word_tokenize(row[keys][1])\n",
    "            else:\n",
    "                del row[keys]\n",
    "    try:\n",
    "        datetime.datetime.strptime(row['creationtime'], \"%Y-%m-%d %H:%M:%S\")\n",
    "    except:\n",
    "        for i in range(10):\n",
    "            try:\n",
    "                row['creationtime']=row['column'+str(i)][2]\n",
    "                break\n",
    "            except:\n",
    "                pass\n",
    "    for keys in list(row):\n",
    "        if keys[:4]=='colu':\n",
    "            row[keys][2]=timeconvert(row[keys][2],row['creationtime'])\n",
    "    row['likecount']=row['likecount'].split('\\n\\n ')[0]\n",
    "    row['username']=row['username'].replace('<font color=\"#0066CC\">',\"\")\n",
    "    row['username']=row['username'].replace('</font>',\"\")\n",
    "    new_cptn=re.sub(r'[\\x80-\\xFF]+','',row['mediacaption'])\n",
    "    if new_cptn!=row['mediacaption']:\n",
    "        row['mediacaption']=re.sub('\\_*','',new_cptn)\n",
    "    row['mediacaption']=word_tokenize(row['mediacaption'].lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "saving-islam",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Load user profile data\n",
    "userinfo = {}    \n",
    "ufile = open(Vine_data_path+\"vine_users_data.json\", 'r', encoding='utf-8')\n",
    "for line in ufile.readlines():\n",
    "    rr = json.loads(line)\n",
    "    userinfo[rr['username'].lower()]=rr['description'].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "changing-cloud",
   "metadata": {
    "code_folding": [
     5
    ]
   },
   "outputs": [],
   "source": [
    "# Construct input session data according to time\n",
    "session_labels=[]\n",
    "session_tokens=[]\n",
    "session_histories=[]\n",
    "session_times=[]\n",
    "for row in dataDict:\n",
    "    if row['question1']=='noneAgg' and row['question2']=='noneBll':\n",
    "        session_labels.append(0)\n",
    "    elif row['question1']=='aggression' and row['question2']=='noneBll':\n",
    "        session_labels.append(0)\n",
    "    else:\n",
    "        session_labels.append(1)\n",
    "    \n",
    "    row_tokens=[]\n",
    "    row_times=[]\n",
    "    row_history=[]\n",
    "    owner_ut=[row['username']]+row['mediacaption']\n",
    "    row_tokens.append(owner_ut)\n",
    "    if row['username'] in userinfo.keys():\n",
    "        row_history.append(userinfo[row['username']])\n",
    "    else:\n",
    "        row_history.append([])\n",
    "    row_times.append(0)\n",
    "    for keys in list(row):\n",
    "        if keys[:4]=='colu':\n",
    "            row_tokens.append([row[keys][0]]+row[keys][1])\n",
    "            row_times.append(row[keys][2])\n",
    "    sorted_row_times=[]\n",
    "    sorted_row_tokens=[]\n",
    "    \n",
    "    mintime=row_times[np.argsort(row_times)[0]]\n",
    "    for i in np.argsort(row_times):\n",
    "        if mintime<0:\n",
    "            sorted_row_times.append(row_times[i]-mintime)\n",
    "        else:\n",
    "            sorted_row_times.append(row_times[i])\n",
    "        sorted_row_tokens.append(row_tokens[i])\n",
    "    \n",
    "    sorted_row_times=sorted_row_times[:MAX_SENTS]+(MAX_SENTS-len(sorted_row_times))*[0]\n",
    "    session_times.append(sorted_row_times)\n",
    "    session_tokens.append(sorted_row_tokens)\n",
    "    session_histories.append(row_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affecting-brain",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Tokenize input sessions\n",
    "embedding_index,lister=generate_indices_embedding(session_tokens)\n",
    "session_indices=generate_word_index(session_tokens,embedding_index)\n",
    "session_history_indices=generate_history_word_index(session_histories,embedding_index)\n",
    "\n",
    "session_indices=np.array(session_indices)\n",
    "session_times=np.array(session_times)\n",
    "session_labels=np.array(session_labels)\n",
    "session_history_indices=np.array(session_history_indices)\n",
    "session_times=np.expand_dims(session_times, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "current-latex",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recreational-disco",
   "metadata": {
    "code_folding": [
     0,
     13,
     83,
     85
    ]
   },
   "outputs": [],
   "source": [
    "class lambdaLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(lambdaLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super(lambdaLayer, self).build(input_shape)  \n",
    "\n",
    "    def call(self, x):\n",
    "        result =x[0]*x[2]+x[1]*x[3]\n",
    "        return result\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[2]\n",
    "class TimeAtt(Layer):\n",
    "    def __init__(self, nb_head, size_per_head, **kwargs):\n",
    "        self.nb_head = nb_head\n",
    "        self.size_per_head = size_per_head\n",
    "        self.output_dim = nb_head*size_per_head\n",
    "        super(TimeAtt, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.WQ = self.add_weight(name='WQ', \n",
    "                                  shape=(input_shape[0][-1], self.output_dim),\n",
    "                                  initializer='glorot_uniform',\n",
    "                                  trainable=True)\n",
    "        self.WK = self.add_weight(name='WK', \n",
    "                                  shape=(input_shape[1][-1], self.output_dim),\n",
    "                                  initializer='glorot_uniform',\n",
    "                                  trainable=True)\n",
    "        self.WV = self.add_weight(name='WV', \n",
    "                                  shape=(input_shape[2][-1], self.output_dim),\n",
    "                                  initializer='glorot_uniform',\n",
    "                                  trainable=True)\n",
    "        \n",
    "        super(TimeAtt, self).build(input_shape)\n",
    "        \n",
    "    def Mask(self, inputs, seq_len, mode='mul'):\n",
    "        if seq_len == None:\n",
    "            return inputs\n",
    "        else:\n",
    "            mask = K.one_hot(seq_len[:,0], K.shape(inputs)[1])\n",
    "            mask = 1 - K.cumsum(mask, 1)\n",
    "            for _ in range(len(inputs.shape)-2):\n",
    "                mask = K.expand_dims(mask, 2)\n",
    "            if mode == 'mul':\n",
    "                return inputs * mask\n",
    "            if mode == 'add':\n",
    "                return inputs - (1 - mask) * 1e12\n",
    "                \n",
    "    def call(self, x):\n",
    "        if len(x) == 4:\n",
    "            Q_seq,K_seq,V_seq,T_seq = x\n",
    "            Q_len,V_len = None,None\n",
    "        elif len(x) == 5:\n",
    "            Q_seq,K_seq,V_seq,Q_len,V_len = x\n",
    "        T1_seq = K.repeat_elements(T_seq,K.int_shape(T_seq)[1],2)\n",
    "        T2_seq = K.permute_dimensions(T1_seq, (0,2,1))\n",
    "        T1_seq = K.reshape(T1_seq, (-1,1,K.shape(T1_seq)[1],K.shape(T1_seq)[2]))\n",
    "        T2_seq = K.reshape(T2_seq, (-1,1,K.shape(T2_seq)[1],K.shape(T2_seq)[2]))\n",
    "        T_seq = add([T1_seq, -T2_seq])\n",
    "        Q_seq = K.dot(Q_seq, self.WQ)\n",
    "        Q_seq = K.reshape(Q_seq, (-1, K.shape(Q_seq)[1], self.nb_head, self.size_per_head))\n",
    "        Q_seq = K.permute_dimensions(Q_seq, (0,2,1,3))\n",
    "        K_seq = K.dot(K_seq, self.WK)\n",
    "        K_seq = K.reshape(K_seq, (-1, K.shape(K_seq)[1], self.nb_head, self.size_per_head))\n",
    "        K_seq = K.permute_dimensions(K_seq, (0,2,1,3))\n",
    "        V_seq = K.dot(V_seq, self.WV)\n",
    "        V_seq = K.reshape(V_seq, (-1, K.shape(V_seq)[1], self.nb_head, self.size_per_head))\n",
    "        V_seq = K.permute_dimensions(V_seq, (0,2,1,3))\n",
    "        A = K.batch_dot(Q_seq, K_seq, axes=[3,3]) / self.size_per_head**0.5\n",
    "        A = K.permute_dimensions(A, (0,3,2,1))\n",
    "        A = self.Mask(A, V_len, 'add')\n",
    "        A = K.permute_dimensions(A, (0,3,2,1))\n",
    "        A = add([T_seq, A])\n",
    "        A = K.softmax(A)\n",
    "        O_seq = K.batch_dot(A, V_seq, axes=[3,2])\n",
    "        O_seq = K.permute_dimensions(O_seq, (0,2,1,3))\n",
    "        O_seq = K.reshape(O_seq, (-1, K.shape(O_seq)[1], self.output_dim))\n",
    "        O_seq = self.Mask(O_seq, Q_len, 'mul')\n",
    "        return O_seq\n",
    "        \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0][0], input_shape[0][1], self.output_dim)\n",
    "def slice(x,index):\n",
    "    return x[:,index,:]\n",
    "class scoreHistory(keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        true = test_session_labels\n",
    "        predictions = model.predict([test_session_indices,test_session_times,test_session_history_indices], batch_size=32, verbose=1)\n",
    "        auc=roc_auc_score(true, predictions[:,1])\n",
    "        predictions = np.argmax(predictions, axis=1)\n",
    "        cr = classification_report(true, predictions,digits=4)\n",
    "        acc_score=accuracy_score(true, predictions)\n",
    "        print(cr)\n",
    "        if times not in results.keys():\n",
    "            results[times]=[float(cr.split()[10]),float(cr.split()[11]),float(cr.split()[12]),auc,0]\n",
    "        else:\n",
    "            if float(cr.split()[12])>results[times][2]:\n",
    "                results[times]=[float(cr.split()[10]),float(cr.split()[11]),float(cr.split()[12]),auc,epoch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quantitative-metro",
   "metadata": {
    "code_folding": [
     1
    ]
   },
   "outputs": [],
   "source": [
    "results={}\n",
    "for times in range(MAX_TIMES):\n",
    "    # Shuffle data\n",
    "    indices = np.arange(len(session_indices))\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    train_session_indices=session_indices[indices[:int(train_valtest_split*len(indices))]]\n",
    "    train_session_times=session_times[indices[:int(train_valtest_split*len(indices))]]\n",
    "    train_session_labels=session_labels[indices[:int(train_valtest_split*len(indices))]]\n",
    "    train_session_history_indices=session_history_indices[indices[:int(train_valtest_split*len(indices))]]\n",
    "\n",
    "    test_session_indices=session_indices[indices[int(train_valtest_split*len(indices)):]]\n",
    "    test_session_times=session_times[indices[int(train_valtest_split*len(indices)):]]\n",
    "    test_session_labels=session_labels[indices[int(train_valtest_split*len(indices)):]]\n",
    "    test_session_history_indices=session_history_indices[indices[int(train_valtest_split*len(indices)):]]\n",
    "        \n",
    "    title_input = Input(shape=(MAX_SENT_LENGTH,), dtype='int32')\n",
    "    wordEmb=Embedding(len(lister),400, weights=[lister],trainable=True)\n",
    "    titles = wordEmb(title_input)\n",
    "    d_titles=Dropout(0.2)(titles)\n",
    "    sentenceGRU=Bidirectional(GRU(32,return_sequences=True))\n",
    "    title_attrep=sentenceGRU(d_titles)\n",
    "    dense1=Dense(32,activation='tanh')\n",
    "    dense2=Dense(1)\n",
    "    attention = dense1(title_attrep)\n",
    "    attention = Flatten()(dense2(attention))\n",
    "    attention_weight = Activation('softmax')(attention)\n",
    "    title_att=keras.layers.Dot((1, 1))([title_attrep, attention_weight])\n",
    "    sentEncodert = Model(title_input,title_att)\n",
    "    \n",
    "    history_input = Input((HISTORY_LENGTH,), dtype='int32')\n",
    "    histories = wordEmb(history_input)\n",
    "    d_histories=Dropout(0.2)(histories)\n",
    "    his_attrep=sentenceGRU(d_histories)\n",
    "    hattention = dense1(his_attrep)\n",
    "    hattention = Flatten()(dense2(hattention))\n",
    "    hattention_weight = Activation('softmax')(hattention)\n",
    "    hisT_att=keras.layers.Dot((1, 1))([his_attrep, hattention_weight])\n",
    "    histEncodert = Model(history_input,hisT_att)\n",
    "    \n",
    "    session_input = Input((MAX_SENTS,MAX_SENT_LENGTH,), dtype='int32')\n",
    "    session_encoded= TimeDistributed(sentEncodert)(session_input)\n",
    "    session_time_input = Input((MAX_SENTS,1), dtype='float32')\n",
    "    time_dense1=Dense(1,bias=True,activation='sigmoid')\n",
    "    time_encoded=time_dense1(session_time_input)\n",
    "    session_history_input = Input((MAX_SENTS,HISTORY_LENGTH,), dtype='int32')\n",
    "    session_history_encoded= TimeDistributed(histEncodert)(session_history_input)\n",
    "    d_session_encoded=Dropout(0.2)(session_encoded)\n",
    "    d_session_history_encoded=Dropout(0.2)(session_history_encoded)\n",
    "    session_rep=Bidirectional(GRU(64,return_sequences=True))(d_session_encoded)\n",
    "    rsession_rep=TimeAtt(1,64)([session_rep,session_rep,session_rep,time_encoded])\n",
    "    his_dense=Dense(1,bias=True)\n",
    "    w_rsession_rep=his_dense(rsession_rep)\n",
    "    w_d_session_history_encoded=his_dense(d_session_history_encoded)\n",
    "    lamb=lambdaLayer()\n",
    "    rsession_rep=lamb([w_rsession_rep,w_d_session_history_encoded,rsession_rep,d_session_history_encoded])\n",
    "    satt= Dense(32,activation='tanh')(rsession_rep)\n",
    "    satt= Flatten()(Dense(1)(satt))\n",
    "    sattention_weight = Activation('softmax')(satt)\n",
    "    session_vec=keras.layers.Dot((1, 1))([rsession_rep, sattention_weight])\n",
    "    predict_vec=Dense(2,activation='softmax')(session_vec)\n",
    "    model = Model([session_input,session_time_input,session_history_input],predict_vec)\n",
    "    model.compile(loss=['categorical_crossentropy'], optimizer=Adam(lr=0.001), metrics=['acc'])\n",
    "    scorehistory = scoreHistory()\n",
    "    model.fit([train_session_indices,train_session_times,train_session_history_indices],to_categorical(train_session_labels),batch_size=BATCH_SIZE,\n",
    "              callbacks=[scorehistory],epochs=MAX_EPOCHS,shuffle=True,class_weight={0:1,1:1})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
